{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Step by Step breakdown of the Word Frequency Spark example\n",
    "\n",
    "In this slide, we will explain the code line by line to help you see the workings of PySpark and also how familiar it can be if you have some prior Python experience. Use the notebook function to run line by line, observe the outcome and feel free to edit if you wish to find out more. Explanations will be provided in-line in your notebook. \n",
    "\n",
    "If you are not familiar with Jupyter Notebook, here is a crash course:\n",
    "- You can move your mouse over the numbers on the left (looks like [1], [2] and so on). The number will turn into a \"Play\" button. Click on that and that particular line of code will be executed.\n",
    "- Sometimes the standard output (things that will get \"printed\" in the console) will be displayed right below your code. Sometimes you will see nothing. Both are OK. What's not OK, is when you received an error output. \n",
    "- If you have assigned value(s) to a variable, you will not see any output in the console. To view, you can just type the name of the variable in the next line (some of the code example has already that done for you). \n",
    "- You can also select Run All to execute all codes at one go (and in sequence), but that's not so fun, is it? :)\n",
    "\n",
    "Enjoy and have fun learning.\n",
    "\n",
    "Have you tried to figure out the previous python code that uses Spark (via pyspark) to perform the exact word frequency calculation tasks you have tested using MapReduce in Week 4? Let's dive in to run through the code line by line. \n",
    "\n",
    "1.0 First of all you begin by importing *SparkSession* from the library *pyspark*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.0 It is important to begin by creating a SparkSession. This can be seen as an entry to Spark (which should be installed on the same machine beforehand). This will allow you to use all functions provided in Spark via pyspark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName('Firstprogram').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a SparkSession is created successfully, you should get the version, name of the master node and the app name printed.\n",
    "\n",
    "You can also update or set configuration that is specific to your needs by using the config command, but for this example we will keep the default configuration.\n",
    "\n",
    "Always end the SparkSession command with `.getOrCreate()`.\n",
    "\n",
    "3.0 Next will be the reading from a file. Let's read the text file you have processed in Week 4. It has been named the same as text.txt. You use the following command to read it into a PythonRDD object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = spark.read.text(\"text.txt\").rdd.map(lambda r: r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the text_file object, and also try to check its contents. Can you see how the text file text.txt is now mapped into a PythonRDD object?\n",
    "1. List item\n",
    "2. List item\n",
    "\n",
    "4.0 Doing the word count...\n",
    "Next, you will convert the text_file object into a flatMap dataframe. What really happens here is each line is split by a space to convert each word into a key value pair of word and 1. The final step in the process was to add up all the 1s. Does this look familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = text_file.flatMap(lambda line: line.split(\" \"))\\\n",
    "                            .map(lambda word: (word, 1))\\\n",
    "                          .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.0 Collection time\n",
    "The next step is to collect the contents from all nodes. Remember, in a cluster setting, the RDDs are executed in more than one nodes so this step is usually required to collect the results from all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.0 Checking the output\n",
    "Finally, you can print the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the constrains on my local, this is the output:\n",
    "\n",
    "Four: 1 \n",
    "<br>score: 1\n",
    "<br>and: 5\n",
    "<br>seven: 1\n",
    "<br>years: 1\n",
    "<br>ago: 1\n",
    "<br>our: 2\n",
    "<br>fathers: 1\n",
    "<br>brought: 1\n",
    "<br>forth: 1\n",
    "<br>on: 2\n",
    "<br>this: 2\n",
    "<br>continent,: 1\n",
    "<br>a: 7\n",
    "<br>new: 2\n",
    "<br>nation,: 3\n",
    "<br>conceived: 2\n",
    "<br>in: 4\n",
    "<br>Liberty,: 1\n",
    "<br>dedicated: 3\n",
    "<br>to: 8\n",
    "<br>the: 9\n",
    "<br>proposition: 1\n",
    "<br>that: 10\n",
    "<br>all: 1\n",
    "<br>men: 1\n",
    "<br>are: 3\n",
    "<br>created: 1\n",
    "<br>equal.: 1\n",
    "<br>: 2\n",
    "<br>Now: 1\n",
    "<br>we: 6\n",
    "<br>engaged: 1\n",
    "<br>great: 3\n",
    "<br>civil: 1\n",
    "<br>war,: 1\n",
    "<br>testing: 1\n",
    "<br>whether: 1\n",
    "<br>or: 2\n",
    "<br>any: 1\n",
    "<br>nation: 2\n",
    "<br>so: 3\n",
    "<br>dedicated,: 1\n",
    "<br>can: 5\n",
    "<br>long: 2\n",
    "<br>endure.: 1\n",
    "<br>We: 2\n",
    "<br>met: 1\n",
    "<br>battle-field: 1\n",
    "<br>of: 5\n",
    "<br>war.: 1\n",
    "<br>have: 5\n",
    "<br>come: 1\n",
    "<br>dedicate: 1\n",
    "<br>portion: 1\n",
    "<br>field,: 1\n",
    "<br>as: 1\n",
    "<br>final: 1\n",
    "<br>resting: 1\n",
    "<br>place: 1\n",
    "<br>for: 5\n",
    "<br>those: 1\n",
    "<br>who: 3\n",
    "<br>here: 5\n",
    "<br>gave: 2\n",
    "<br>their: 1\n",
    "<br>lives: 1\n",
    "<br>might: 1\n",
    "<br>live.: 1\n",
    "<br>It: 3\n",
    "<br>is: 3\n",
    "<br>altogether: 1\n",
    "<br>fitting: 1\n",
    "<br>proper: 1\n",
    "<br>should: 1\n",
    "<br>do: 1\n",
    "<br>this.: 1\n",
    "<br>But,: 1\n",
    "<br>larger: 1\n",
    "<br>sense,: 1\n",
    "<br>not: 5\n",
    "<br>dedicate—we: 1\n",
    "<br>consecrate—we: 1\n",
    "<br>hallow—this: 1\n",
    "<br>ground.: 1\n",
    "<br>The: 2\n",
    "<br>brave: 1\n",
    "<br>men,: 1\n",
    "<br>living: 1\n",
    "<br>dead,: 1\n",
    "<br>struggled: 1\n",
    "<br>here,: 2\n",
    "<br>consecrated: 1\n",
    "<br>it,: 1\n",
    "<br>far: 2\n",
    "<br>above: 1\n",
    "<br>poor: 1\n",
    "<br>power: 1\n",
    "<br>add: 1\n",
    "<br>detract.: 1\n",
    "<br>world: 1\n",
    "<br>will: 1\n",
    "<br>little: 1\n",
    "<br>note,: 1\n",
    "<br>nor: 1\n",
    "<br>remember: 1\n",
    "<br>what: 2\n",
    "<br>say: 1\n",
    "<br>but: 1\n",
    "<br>it: 1\n",
    "<br>never: 1\n",
    "<br>forget: 1\n",
    "<br>they: 3\n",
    "<br>did: 1\n",
    "<br>here.: 1\n",
    "<br>us: 2\n",
    "<br>living,: 1\n",
    "<br>rather,: 1\n",
    "<br>be: 2\n",
    "<br>unfinished: 1\n",
    "<br>work: 1\n",
    "<br>which: 2\n",
    "<br>fought: 1\n",
    "<br>thus: 1\n",
    "<br>nobly: 1\n",
    "<br>advanced.: 1\n",
    "<br>rather: 1\n",
    "<br>task: 1\n",
    "<br>remaining: 1\n",
    "<br>before: 1\n",
    "<br>us—that: 1\n",
    "<br>from: 2\n",
    "<br>these: 2\n",
    "<br>honored: 1\n",
    "<br>dead: 2\n",
    "<br>take: 1\n",
    "<br>increased: 1\n",
    "<br>devotion: 1\n",
    "<br>cause: 1\n",
    "<br>last: 1\n",
    "<br>full: 1\n",
    "<br>measure: 1\n",
    "<br>devotion—that: 1\n",
    "<br>highly: 1\n",
    "<br>resolve: 1\n",
    "<br>shall: 3\n",
    "<br>died: 1\n",
    "<br>vain—that: 1\n",
    "<br>under: 1\n",
    "<br>God,: 1\n",
    "<br>birth: 1\n",
    "<br>freedom—and: 1\n",
    "<br>government: 1\n",
    "<br>people,: 3\n",
    "<br>by: 1\n",
    "<br>perish: 1\n",
    "<br>earth.: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please visit [Google Colab: 5.3.2 Step by Step breakdown of the Word Frequency Spark example](https://colab.research.google.com/drive/1lRf_wySa7JU3uDUldWi1X1rZ9UVFp289?usp=sharing) for a smooth process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
